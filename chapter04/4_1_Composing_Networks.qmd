---
title: "4.1 Composing Networks"
author: "CF Wang"
date: "2026-01-18"
format: html
---

```{r}
#| label: setup
#| include: false
#| fig-width: 6
#| fig-asp: 0.618
#| fig-align: center
#| out-width: 70%

library(ggplot2)
library(patchwork)
```

The purpose of this notebook is to understand what happens when we feed one neural network into another. It works through an example similar to 4.1 and varies both networks

```{r}
#| label: ReLU

# Define the Rectified Linear Unit (ReLU) function
ReLU <- function(preactivation) {
  ifelse(preactivation > 0, preactivation, 0)
}
```

```{r}
#| label: shallow_1_1_3

# Define a shallow neural network with, one input, one output, and three hidden units
shallow_1_1_3 <- function(x, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31) {
  # Initial lines
  pre_1 <- theta_10 + theta_11 * x
  pre_2 <- theta_20 + theta_21 * x
  pre_3 <- theta_30 + theta_31 * x
  # Activation functions
  act_1 <- activation_fn(pre_1)
  act_2 <- activation_fn(pre_2)
  act_3 <- activation_fn(pre_3)
  # Weight activations
  w_act_1 <- phi_1 * act_1
  w_act_2 <- phi_2 * act_2
  w_act_3 <- phi_3 * act_3
  # Combine weighted activation and add y offets
  y <- phi_0 + w_act_1 + w_act_2 + w_act_3
  # Return everything we have calculated
  data.frame(x = x, y = y, pre1 = pre_1, pre2 = pre_2, pre3 = pre_3, act1 = act_1, act2 = act_2, act3 = act_3, wact1 = w_act_1, wact2 = w_act_2, wact3 = w_act_3)
}
```

```{r}
#| label: plot-neural

# Plot two shallow neural networks and the composition of the two
plot_neural_two_components <- function(x_in, net1_out, net2_out, net12_out = NULL) {
  p1 <- ggplot() +
    xlim(c(-1, 1)) + ylim(c(-1, 1)) +
    xlab("Net 1 input") + ylab("Net 1 output") +
    geom_line(
      mapping = aes(x = x_in, y = net1_out),
      color = "red3",
      linewidth = 0.75)
  p2 <- ggplot() +
    xlim(c(-1, 1)) + ylim(c(-1, 1)) +
    xlab("Net 2 input") + ylab("Net 2 output") +
    geom_line(
      mapping = aes(x = x_in, y = net2_out),
      color = "blue3",
      linewidth = 0.75)
  
  if (!is.null(net12_out)) {
    p3 <- ggplot() +
      xlim(c(-1, 1)) + ylim(c(-1, 1)) +
      xlab("Net 1 input") + ylab("Net 2 output") +
      geom_line(
        mapping = aes(x = x_in, y = net12_out),
        color = "green3",
        linewidth = 0.75)
    grid_layout <- "
    AB
    C#
    "
    p1 + p2 + p3 +
      plot_layout(design = grid_layout)
  } else {
    p1 + p2
  }
}
```

Let's define two networks. We'll put the prefixes n1_ and n2_ before all the variables to make it clear which network is which. We'll just consider the inputs and outputs over the range [-1,1].

```{r}
#| label: net1-and-net2
#| fig-width: 8
#| fig-height: 4

# Now we'll change things a up a bit.  What happens if we change the second network? (note the *-1 change)

# Now lets define some parameters and run the first neural network
n1_theta_10 <-  0.0;  n1_theta_11 <- -1.0
n1_theta_20 <-  0;    n1_theta_21 <-  1.0
n1_theta_30 <- -0.67; n1_theta_31 <-  1.0
n1_phi_0 <- 1.0; n1_phi_1 <- -2.0; n1_phi_2 <- -3.0; n1_phi_3 <- 9.3

# Now lets define some parameters and run the second neural network
n2_theta_10 <- -0.6; n2_theta_11 <- -1.0
n2_theta_20 <-  0.2; n2_theta_21 <-  1.0
n2_theta_30 <- -0.5; n2_theta_31 <-  1.0
n2_phi_0 <- 0.5; n2_phi_1 <- -1.0; n2_phi_2 <- -1.5; n2_phi_3 <- 2.0

# Display the two inputs
x <- seq(-1, 1, 0.001)

# We run the first and second neural networks for each of these input values
net1_out <- shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
net2_out <- shallow_1_1_3(x, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

# Plot both graphs
plot_neural_two_components(x, net1_out$y, net2_out$y)
```

```{r}
# TODO
# Take a piece of paper and draw what you think will happen when we feed the
# output of the first network into the second one.  Draw the relationship between
# the input of the first network and the output of the second one.
```

```{r}
#| label: net1, net2 and net12
#| fig-width: 8
#| fig-asp: 1

# Now let's see if your predictions were right

# TODO feed the output of first network into second network (replace this line)
net2_input <- net1_out$y
net12_out <- shallow_1_1_3(net2_input, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

# Plot all three graphs
plot_neural_two_components(x, net1_out$y, net2_out$y, net12_out$y)
```

```{r}
#| fig-width: 8
#| fig-height: 4

# Now we'll change things a up a bit.  What happens if we change the second network? (note the *-1 change)
net1_out <- shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
net2_out <- shallow_1_1_3(x, ReLU, n2_phi_0, n2_phi_1*-1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)
plot_neural_two_components(x, net1_out$y, net2_out$y)
```

```{r}
#| fig-width: 8
#| fig-asp: 1

# When you have a prediction, run this code to see if you were right
net12_out <- shallow_1_1_3(net1_out$y, ReLU, n2_phi_0, n2_phi_1*-1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)
plot_neural_two_components(x, net1_out$y, net2_out$y, net12_out$y)
```

```{r}
#| fig-width: 8
#| fig-height: 4

# Let's change things again.  What happens if we change the first network? (note the changes)
net1_out <- shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1*0.5, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
net2_out <- shallow_1_1_3(x, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)
plot_neural_two_components(x, net1_out$y, net2_out$y)
```

```{r}
#| fig-width: 8
#| fig-asp: 1

# TODO
# Take a piece of paper and draw what you think will happen when we feed the
# output of the modified first network into the original second network.  Draw the relationship between
# the input of the first network and the output of the second one.
     
# When you have a prediction, run this code to see if you were right
net12_out <- shallow_1_1_3(net1_out$y, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)
plot_neural_two_components(x, net1_out$y, net2_out$y, net12_out$y)
``` 

```{r}
#| fig-width: 8
#| fig-height: 4

# Let's change things again.  What happens if the first network and second networks are the same?
net1_out <- shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
net2_out_new <- shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
plot_neural_two_components(x, net1_out$y, net2_out_new$y)
```

```{r}
#| fig-width: 8
#| fig-asp: 1

# TODO
# Take a piece of paper and draw what you think will happen when we feed the
# output of the first network into the a copy of itself.  Draw the relationship between
# the input of the first network and the output of the second one.

# When you have a prediction, run this code to see if you were right
net12_out <- shallow_1_1_3(net1_out$y, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
plot_neural_two_components(x, net1_out$y, net2_out_new$y, net12_out$y)
```

```{r}
#| fig-width: 8
#| fig-asp: 1

# TODO
# Contemplate what you think will happen when we feed the
# output of the original first network into a second copy of the original first network, and then
# the output of that into the original second network (so now we have a three layer network)
# How many total linear regions will we have in the output?
net123_out <- shallow_1_1_3(net12_out$y, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)
plot_neural_two_components(x, net12_out$y, net2_out$y, net123_out$y)
```

```{r}
# TODO
# How many linear regions would there be if we ran N copies of the first network, feeding the result of the first
# into the second, the second into the third and so on, and then passed the result into the original second
# network (blue curve above)

# Take away conclusion:  with very few parameters, we can make A LOT of linear regions, but
# they depend on one another in complex ways that quickly become too difficult to understand intuitively.
```


