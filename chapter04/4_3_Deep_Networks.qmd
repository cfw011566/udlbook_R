---
title: "4.3 Deep Networks"
author: "CF Wang"
date: "2025-12-25"
format: html
---

```{r}
#| label: setup
#| echo: true
#| fig-width: 8
#| fig-height: 6
#| out-width: 80%
#| fig-align: center
```

```{r}
#| label: load library
#| echo: false
library(ggplot2)
```

```{r}
#| label: ReLU
# Define the Rectified Linear Unit (ReLU) function
ReLU <- function(preactivation) {
  ifelse(preactivation > 0, preactivation, 0)
}
```

```{r}
#| label: shallow_1_1_3
#| echo: false
# Define a shallow neural network with, one input, one output, and three hidden units
shallow_1_1_3 <- function(x, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31) {
  # Initial lines
  pre_1 <- theta_10 + theta_11 * x
  pre_2 <- theta_20 + theta_21 * x
  pre_3 <- theta_30 + theta_31 * x
  # Activation functions
  act_1 <- activation_fn(pre_1)
  act_2 <- activation_fn(pre_2)
  act_3 <- activation_fn(pre_3)
  # Weight activations
  w_act_1 <- phi_1 * act_1
  w_act_2 <- phi_2 * act_2
  w_act_3 <- phi_3 * act_3
  # Combine weighted activation and add y offets
  y <- phi_0 + w_act_1 + w_act_2 + w_act_3
  # Return everything we have calculated
  return(data.frame(x = x, y = y, pre1 = pre_1, pre2 = pre_2, pre3 = pre_3, act1 = act_1, act2 = act_2, act3 = act_3, wact1 = w_act_1, wact2 = w_act_2, wact3 = w_act_3))
}
```

```{r}
#| label: plot neural
#| echo: false

# # Plot the shallow neural network.  We'll assume input in is range [-1,1] and output [-1,1]
#def plot_neural(x, y):
#  fig, ax = plt.subplots()
#  ax.plot(x.T,y.T)
#  ax.set_xlabel('Input'); ax.set_ylabel('Output')
#  ax.set_xlim([-1,1]);ax.set_ylim([-1,1])
#  ax.set_aspect(1.0)
#  plt.show()
# Plot two shallow neural networks and the composition of the two

plot_neural <- function(x, y) {
  par(xaxs = "i", yaxs = "i")
  plot(x, y, type = "l", col = "skyblue3",
       lwd = 2, las = 1,
       xlim = c(-1, 1), ylim = c(-1, 1),
       xaxp = c(-1, 1, 8),
       yaxp = c(-1, 1, 8),
       xlab = "Input", ylab = "Output")
}
```

Let's define a network. We'll just consider the inputs and outputs over the range [-1,1].

```{r}
#| fig-width: 8
#| fig-height: 8

# Now lets define some parameters and run the first neural network
n1_theta_10 <-  0.0;  n1_theta_11 <- -1.0
n1_theta_20 <-  0.0;  n1_theta_21 <-  1.0
n1_theta_30 <- -0.67; n1_theta_31 <-  1.0
n1_phi_0 <- 1.0; n1_phi_1 <- -2.0; n1_phi_2 <- -3.0; n1_phi_3 <- 9.3

# Define a range of input values
n1_in <- seq(-1, 1, 0.01)

# We run the neural network for each of these input values
n1_out <- shallow_1_1_3(n1_in, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)
# And then plot it
plot_neural(n1_in, n1_out$y)
```

Now we'll define the same neural network, but this time, we will use matrix form as in equation 4.15. When you get this right, it will draw the same plot as above.

```{r}
#| fig-width: 8
#| fig-height: 8

beta_0 <- matrix(0, nrow = 3, ncol = 1)
Omega_0 <- matrix(0, nrow = 3, ncol = 1)
beta_1 <- matrix(0, nrow = 1, ncol = 1)
Omega_1 <- matrix(0, nrow = 1, ncol = 3)

# TODO Fill in the values of the beta and Omega matrices with the n1_theta and n1_phi parameters that define the network above
# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0]
# To get you started I've filled in a couple:
beta_0[1,1] <- n1_theta_10; beta_0[2,1] <- n1_theta_20; beta_0[3,1] <- n1_theta_30
Omega_0[1,1] <- n1_theta_11; Omega_0[2,1] <- n1_theta_21; Omega_0[3,1] <- n1_theta_31
beta_1[1,1] <- n1_phi_0;
Omega_1[1,1] <- n1_phi_1; Omega_1[1,2] <- n1_phi_2; Omega_1[1,3] <- n1_phi_3

# Make sure that input data matrix has different inputs in its columns
#n_data = n1_in.size
#n_dim_in = 1
#n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))

n_data = length(n1_in)
n_dim_in = 1
n1_in_mat <- matrix(n1_in, nrow = n_dim_in, ncol = n_data)

# This runs the network for ALL of the inputs, x at once so we can draw graph
#h1 = ReLU(beta_0 + np.matmul(Omega_0,n1_in_mat))
#n1_out = beta_1 + np.matmul(Omega_1,h1)
h1 <- ReLU(c(beta_0) + Omega_0 %*% n1_in_mat)
n1_out = c(beta_1) + Omega_1 %*% h1
n1_out = c(n1_out)

# Draw the network and check that it looks the same as the non-matrix case
plot_neural(n1_in, n1_out)
```

```{r}
#| fig-width: 8
#| fig-height: 8

# Now lets define some parameters and run the second neural network
n2_theta_10 <- -0.6; n2_theta_11 <- -1.0
n2_theta_20 <-  0.2; n2_theta_21 <-  1.0
n2_theta_30 <- -0.5; n2_theta_31 <-  1.0
n2_phi_0 <- 0.5; n2_phi_1 <- -1.0; n2_phi_2 <- -1.5; n2_phi_3 <- 2.0

# Define a range of input values
n2_in <- seq(-1, 1, 0.01)

# We run the second neural network on the output of the first network
n2_out = shallow_1_1_3(n1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)
# And then plot it
plot_neural(n1_in, n2_out$y)
```


```{r}
#| fig-width: 8
#| fig-height: 8

beta_0 <- matrix(0, nrow = 3, ncol = 1)
Omega_0 <- matrix(0, nrow = 3, ncol = 1)
beta_1 <- matrix(0, nrow = 3, ncol = 1)
Omega_1 <- matrix(0, nrow = 3, ncol = 3)
beta_2 <- matrix(0, nrow = 1, ncol = 1)
Omega_2 <- matrix(0, nrow = 1, ncol = 3)

# TODO Fill in the values of the beta and Omega matrices for the n1_theta, n1_phi, n2_theta, and n2_phi parameters
# that define the composition of the two networks above (see eqn 4.5 for Omega1 and beta1 albeit in different notation)
# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0] SO EVERYTHING IS OFFSET
# To get you started I've filled in a few:
beta_0[1,1] <- n1_theta_10; beta_0[2,1] <- n1_theta_20; beta_0[3,1] <- n1_theta_30
Omega_0[1,1] <- n1_theta_11; Omega_0[2,1] <- n1_theta_21; Omega_0[3,1] <- n1_theta_31
beta_1[1,1] <- n2_theta_10 + n2_theta_11 * n1_phi_0; beta_1[2,1] <- n2_theta_20 + n2_theta_21 * n1_phi_0; beta_1[3,1] <- n2_theta_30 + n2_theta_31 * n1_phi_0
Omega_1[1,1] <- n2_theta_11 * n1_phi_1; Omega_1[1,2] <- n2_theta_11 * n1_phi_2; Omega_1[1,3] <- n2_theta_11 * n1_phi_3
Omega_1[2,1] <- n2_theta_21 * n1_phi_1; Omega_1[2,2] <- n2_theta_21 * n1_phi_2; Omega_1[2,3] <- n2_theta_21 * n1_phi_3
Omega_1[3,1] <- n2_theta_31 * n1_phi_1; Omega_1[3,2] <- n2_theta_31 * n1_phi_2; Omega_1[3,3] <- n2_theta_31 * n1_phi_3
beta_2[1,1] <- n2_phi_0;
Omega_2[1,1] <- n2_phi_1; Omega_2[1,2] <- n2_phi_2; Omega_2[1,3] <- n2_phi_3

# Make sure that input data matrix has different inputs in its columns
n_data = length(n1_in)
n_dim_in = 1
n1_in_mat = matrix(n1_in, nrow = n_dim_in, ncol = n_data)

# This runs the network for ALL of the inputs, x at once so we can draw graph (hence extra np.ones term)
h1 <- ReLU(c(beta_0) + Omega_0 %*% n1_in_mat)
h2 <- ReLU(c(beta_1) + Omega_1 %*% h1)
n1_out = c(beta_2) + Omega_2 %*% h2
n1_out = c(n1_out)

# Draw the network and check that it looks the same as the non-matrix version
plot_neural(n1_in, n1_out)
```

Now let's make a deep network with 3 hidden layers. It will have $D_i=4$ inputs, $D_1=5$ neurons in the first layer, $D_2=2$ neurons in the second layer and $D_3=4$ neurons in the third layer, and $D_o=1$ output. Consult figure 4.6 and equations 4.15 for guidance.

```{r}
set.seed(123)
# define sizes
D_i <- 4; D_1 <- 5; D_2 <- 2; D_3 <- 4; D_o <- 1
# We'll choose the inputs and parameters of this network randomly using rnorm
# For example, we'll set the input using
n_data <- 4;
x <- matrix(rnorm(D_i*n_data), nrow = D_i, ncol = n_data)
# TODO initialize the parameters randomly with the correct sizes
# Replace the lines below
beta_0 <- matrix(rnorm(D_1*1), nrow = D_1, ncol = 1)
Omega_0 <- matrix(rnorm(D_1*D_i), nrow = D_1, ncol = D_i)
beta_1 <- matrix(rnorm(D_2*1), nrow = D_2, ncol = 1)
Omega_1 <- matrix(rnorm(D_2*D_1), nrow = D_2, ncol = D_1)
beta_2 <- matrix(rnorm(D_3*1), nrow = D_3, ncol = 1)
Omega_2 <- matrix(rnorm(D_3*D_2), nrow = D_3, ncol = D_2)
beta_3 <- matrix(rnorm(D_o*1), nrow = D_o, ncol = 1)
Omega_3 <- matrix(rnorm(D_o*D_3), nrow = D_o, ncol = D_3)

# If you set the parameters to the correct sizes, the following code will run
h1 = ReLU(c(beta_0) + Omega_0 %*% x)
h2 = ReLU(c(beta_1) + Omega_1 %*% h1)
h3 = ReLU(c(beta_2) + Omega_2 %*% h2)
y = c(beta_3) + Omega_3 %*% h3

if (all(dim(h1) != c(D_1, n_data))) {
  print("h1 is wrong shape")
}
if (all(dim(h2) != c(D_2, n_data))) {
  print("h2 is wrong shape")
}
if (all(dim(h3) != c(D_3, n_data))) {
  print("h3 is wrong shape")
}
if (all(dim(y) != c(D_o, n_data))) {
  print("Output is wrong shape")
}

# Print the inputs and outputs
print("Input data points")
print(x)
print ("Output data points")
print(y)
```

