---
title: "3.2 -- Shallow neural networks II"
author: "CF Wang"
date: "2026-01-18"
format: html
---

```{r}
#| label: setup
#| fig-width: 6
#| fig-asp: 0.618
#| fig-align: center
#| out-width: 70%

library(ggplot2)
library(patchwork)
```

The purpose of this notebook is to gain some familiarity with shallow neural networks with 2D inputs. It works through an example similar to figure 3.8 and experiments with different activation functions.

```{r}
#| label: plot-2D-neural-network

# Code to draw 2D function -- read it so you know what is going on, but you don't have to change it
draw_2D_function <- function(x1, x2, y, title, add = FALSE) {
  df <- expand.grid(x = x1, y = x2)
  df$z <- y 
  ggplot(data = df, mapping = aes(x = x, y = y, z = z)) +
    xlim(0, 10) +
    ylim(0, 10) +
    xlab("x1") +
    ylab("x2") +
    labs(title = title) +
    scale_fill_viridis_d(option = "plasma") +
    geom_contour_filled(
      show.legend = FALSE,
      bins = 128) +
    geom_contour(
      show.legend = FALSE,
      bins = 20,
      color = "grey",
      linewidth = 0.25)
}

# Plot the shallow neural network.  We'll assume input in is range [0,10],[0,10] and output [-10,10]
plot_neural_2_inputs <- function(x1, x2, nn_result) {
  p1 <- draw_2D_function(x1, x2, nn_result$pre_1, "Preactivation")
  p2 <- draw_2D_function(x1, x2, nn_result$pre_2, "Preactivation")
  p3 <- draw_2D_function(x1, x2, nn_result$pre_3, "Preactivation")
  p4 <- draw_2D_function(x1, x2, nn_result$act_1, "Activation")
  p5 <- draw_2D_function(x1, x2, nn_result$act_2, "Activation")
  p6 <- draw_2D_function(x1, x2, nn_result$act_3, "Activation")
  p7 <- draw_2D_function(x1, x2, nn_result$w_act_1, "Weighted Act")
  p8 <- draw_2D_function(x1, x2, nn_result$w_act_2, "Weighted Act")
  p9 <- draw_2D_function(x1, x2, nn_result$w_act_3, "Weighted Act")
    
  p0 <-draw_2D_function(x1, x2, nn_result$y, "Network output, y")
  
  grid_layout <- "
  ABC
  DEF
  GHI
  JJ#
  JJ#
  "
  p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p0 +
    plot_layout(design = grid_layout)
}
```

```{r}
#| label: ReLU-function

# Define the Rectified Linear Unit (ReLU) function
ReLU <- function(preactivation) {
  ifelse(preactivation > 0, preactivation, 0)
}
```

```{r}
#| label: shallow_2_1_3

# Define a shallow neural network with, two input, one output, and three hidden units
shallow_2_1_3 <- function(x1, x2, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32) {
  df_nn <- expand.grid(x1 = x1, x2 = x2)
  # TODO Replace the lines below to compute the three initial linear functions
  # (figure 3.8a-c) from the theta parameters.  These are the preactivations
  df_nn$pre_1 <- theta_10 + theta_11 * df_nn$x1 + theta_12 * df_nn$x2
  df_nn$pre_2 <- theta_20 + theta_21 * df_nn$x1 + theta_22 * df_nn$x2
  df_nn$pre_3 <- theta_30 + theta_31 * df_nn$x1 + theta_32 * df_nn$x2

  # Pass these through the ReLU function to compute the activations as in
  # figure 3.8 d-f
  df_nn$act_1 <- activation_fn(df_nn$pre_1)
  df_nn$act_2 <- activation_fn(df_nn$pre_2)
  df_nn$act_3 <- activation_fn(df_nn$pre_3)

  # TODO Replace the code below to weight the activations using phi1, phi2 and phi3
  # To create the equivalent of figure 3.8 g-i
  df_nn$w_act_1 <- phi_1 * df_nn$act_1
  df_nn$w_act_2 <- phi_2 * df_nn$act_2
  df_nn$w_act_3 <- phi_3 * df_nn$act_3

  # TODO Replace the code below to combing the weighted activations and add
  # phi_0 to create the output as in figure 3.8j
  df_nn$y <- phi_0 + df_nn$w_act_1 + df_nn$w_act_2 + df_nn$w_act_3

  # Return everything we have calculated
  df_nn
}
```

```{r}
#| label: run-shallow_2_1_3-network
#| fig-width: 8
#| fig-asp: 1.5

# Now lets define some parameters and run the neural network
theta_10 <- -4.0; theta_11 <-  0.9; theta_12 <-  0.0
theta_20 <-  5.0; theta_21 <- -0.9; theta_22 <- -0.5
theta_30 <- -7;   theta_31 <-  0.5; theta_32 <-  0.9
phi_0 <- 0.0; phi_1 <- -2.0; phi_2 <- 2.0; phi_3 <- 1.5

x1 <- seq(0.0, 10.0, 0.1)
x2 <- seq(0.0, 10.0, 0.1)
# x1, x2 = np.meshgrid(x1, x2)  # https://www.geeksforgeeks.org/numpy-meshgrid-function/

# We run the neural network for each of these input values
nn_result <- shallow_2_1_3(x1, x2, ReLU, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32)

# And then plot it
# plot_neural_2_inputs(x1, x2, nn_result)

plot_neural_2_inputs(x1, x2, nn_result)
```

How many different linear polytopes are made by this model? Identify each in the network output.

Now we'll extend this model to have two outputs $y_1$ and $y_2$, each of which can be visualized with a separate heatmap. You will now have sets of parameters $\phi_{10}, \phi_{11}, \phi_{12}, \phi_{13}$ and $\phi_{20}, \phi_{21}, \phi_{22}, \phi_{23}$ that correspond to each of these outputs.

```{r}
#| label: plot_neural_2_inputs_2_outputs
#
# Plot the shallow neural network.  We'll assume input in is range [0,10],[0,10] and output [-10,10]
plot_neural_2_inputs_2_outputs <- function(x1, x2, nn_result) {
  p1 <- draw_2D_function(x1, x2, nn_result$pre_1, "Preactivation")
  p2 <- draw_2D_function(x1, x2, nn_result$pre_2, "Preactivation")
  p3 <- draw_2D_function(x1, x2, nn_result$pre_3, "Preactivation")
  p4 <- draw_2D_function(x1, x2, nn_result$act_1, "Activation")
  p5 <- draw_2D_function(x1, x2, nn_result$act_2, "Activation")
  p6 <- draw_2D_function(x1, x2, nn_result$act_3, "Activation")
  p7 <- draw_2D_function(x1, x2, nn_result$w_act_11, "Weighted Act 1")
  p8 <- draw_2D_function(x1, x2, nn_result$w_act_12, "Weighted Act 1")
  p9 <- draw_2D_function(x1, x2, nn_result$w_act_13, "Weighted Act 1")
  p10 <- draw_2D_function(x1, x2, nn_result$w_act_21, "Weighted Act 2")
  p11 <- draw_2D_function(x1, x2, nn_result$w_act_22, "Weighted Act 2")
  p12 <- draw_2D_function(x1, x2, nn_result$w_act_23, "Weighted Act 2")
    
  p20 <-draw_2D_function(x1, x2, nn_result$y1, "Network output, y1")
  p21 <-draw_2D_function(x1, x2, nn_result$y2, "Network output, y2")
  
  grid_layout <- "
  ABC
  DEF
  GHI
  JKL
  MM#
  MM#
  MM#
  NN#
  NN#
  NN#
  "
  p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + p20 + p21 +
    plot_layout(design = grid_layout)
}
```

```{r}
#| label: shallow_2_2_3

# Define a shallow neural network with, two inputs, two outputs, and three hidden units
shallow_2_2_3 <- function(x1, x2, activation_fn, phi_10, phi_11, phi_12, phi_13, phi_20, phi_21, phi_22, phi_23, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32) {
  df <- expand.grid(x1 = x1, x2 = x2)
  # TODO -- write this function -- replace the dummy code below
  df$pre_1 <- theta_10 + theta_11 * df$x1 + theta_12 * df$x2
  df$pre_2 <- theta_20 + theta_21 * df$x1 + theta_22 * df$x2
  df$pre_3 <- theta_30 + theta_31 * df$x1 + theta_32 * df$x2
  df$act_1 <- activation_fn(df$pre_1)
  df$act_2 <- activation_fn(df$pre_2)
  df$act_3 <- activation_fn(df$pre_3)
  df$w_act_11 <- phi_11 * df$act_1
  df$w_act_12 <- phi_12 * df$act_2
  df$w_act_13 <- phi_13 * df$act_3
  df$w_act_21 <- phi_21 * df$act_1
  df$w_act_22 <- phi_22 * df$act_2
  df$w_act_23 <- phi_23 * df$act_3
  df$y1 <- phi_10 + df$w_act_11 + df$w_act_12 + df$w_act_13
  df$y2 <- phi_20 + df$w_act_21 + df$w_act_22 + df$w_act_23

  # Return everything we have calculated
  df
}
```

```{r}
#| label: run-shallow_2_2_3-network
#| fig-width: 8
#| fig-asp: 2.0

# Now lets define some parameters and run the neural network
theta_10 <- -4.0; theta_11 <-  0.9; theta_12 <-  0.0
theta_20 <-  5.0; theta_21 <- -0.9; theta_22 <- -0.5
theta_30 <- -7;   theta_31 <-  0.5; theta_32 <-  0.9
phi_10 <-  0.0; phi_11 <- -2.0; phi_12 <-  2.0; phi_13 <- 1.5
phi_20 <- -2.0; phi_21 <- -1.0; phi_22 <- -2.0; phi_23 <- 0.8

x1 <- seq(0.0, 10.0, 0.1)
x2 <- seq(0.0, 10.0, 0.1)

# We run the neural network for each of these input values
nn_result <- shallow_2_2_3(x1, x2, ReLU, phi_10, phi_11, phi_12, phi_13, phi_20, phi_21, phi_22, phi_23, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32)

# And then plot it
plot_neural_2_inputs_2_outputs(x1, x2, nn_result)
```
