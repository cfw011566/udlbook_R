---
title: "2.1 Supervised Learning"
author: "CF Wang"
date: "2026-01-17"
format: html
---

```{r}
#| label: setup
#| fig-width: 6
#| fig-asp: 0.618
#| fig-align: center
#| out-width: 70%

library(ggplot2)
```

```{r}
#| label: input-output- data
 
# Create some input / output data
x <- c(0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90)
y <- c(0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6)

print(x)
print(y)
```

```{r}
#| label: 1D-linear-regression-model

# Define 1D linear regression model
f <- function(x, phi0, phi1) {
  # Replace this line with the linear regression model (eq 2.4)
  phi0 + phi1 * x
}
```

```{r}
#| label: plot-function

# Function to help plot the data
plot <- function(x, y, phi0, phi1) {
  x_line <- seq(0, 2, 0.01)
  y_line <- f(x_line, phi0, phi1)
  ggplot() +
    xlim(c(0, 2)) +
    ylim(c(0, 2)) +
    xlab(bquote("Input, "~x)) +
    ylab(bquote("Onput, "~y)) +
    geom_point(
      data = data.frame(x = x, y = y),
      mapping = aes(x = x, y = y),
      color = "darkblue") +
    geom_line(
      data = data.frame(x = x_line, y = y_line),
      mapping = aes(x = x, y = y),
      color = "blue",
      linewidth = 0.5)
}
```

```{r}
#| label: figure-2.2b

# Set the intercept and slope as in figure 2.2b
phi0 <- 0.4
phi1 <- 0.2
# Plot the data and the model
plot(x, y, phi0, phi1)
```

```{r}
#| label: calculate-loss

# Function to calculate the loss
compute_loss <- function(x, y, phi0, phi1) {
  # TODO Replace this line with the loss calculation (equation 2.5)
  sum((f(x, phi0, phi1) - y)^2)
}
```

```{r}
#| label: test-loss-function

# Compute the loss for our current model
loss <- compute_loss(x, y, phi0, phi1)
sprintf("Your Loss = %.2f, Ground truth = 7.07", loss)
```

```{r}
#| label: loss-of-figure-2.2c

# Set the intercept and slope as in figure 2.2c
phi0 <- 1.60
phi1 <- -0.8
# Plot the data and the model
plot(x, y, phi0, phi1)
loss <- compute_loss(x, y, phi0, phi1)
sprintf("Your Loss = %.2f, Ground truth = 10.28", loss)
```

```{r}
#| label: fit-the-model

# TODO -- Change the parameters manually to fit the model
# First fix phi1 and try changing phi0 until you can't make the loss go down any more
# Then fix phi0 and try changing phi1 until you can't make the loss go down any more
# Repeat this process until you find a set of parameters that fit the model as in figure 2.2d
# You can either do this by hand, or if you want to get fancy, write code to descent automatically in this way
# Start at these values:
phi0 <- 0.85
phi1 <- 0.5

plot(x, y, phi0, phi1)
sprintf("Your Loss = %.2f", compute_loss(x, y, phi0, phi1))
```

# Visualizing the loss function

The above process is equivalent to descending coordinate wise on the loss function

Now let's plot that function

```{r}
#| label: loss-function-as-heatmap

# Plot the loss function as a heatmap
phi0_range <- seq(0.0, 2.0, 0.02)
phi1_range <- seq(-1.0, 1.0, 0.02)
all_losses <- outer(phi0_range, phi1_range)
for (i in 1:length(phi0_range)) {
  for (j in 1:length(phi1_range))
    all_losses[i, j] = compute_loss(x, y, phi0_range[i], phi1_range[j])
}
# all_losses <- outer(phi0_range, phi1_range, function(p0, p1) { compute_loss(x, y, p0, p1)})

# Plot the position of your best fitting line on the loss function
# It should be close to the minimum

df <- expand.grid(phi0 = phi0_range, phi1 = phi1_range)
df$loss <- c(all_losses)
ggplot() +
  xlim(0, 2) +
  ylim(-1, 1) +
  xlab(expression("Intercep, "*phi[0])) +
  ylab(expression("Slope, "*phi[1])) +
  geom_contour_filled(
    data = df,
    mapping = aes(x = phi0, y = phi1, z = loss),
    show.legend = FALSE,
    bins = 256) +
  geom_contour(
    data = df,
    mapping = aes(x = phi0, y = phi1, z = loss),
    show.legend = FALSE,
    bins = 40,
    color = "grey",
    linewidth = 0.25) +
  geom_point(
    data = data.frame(x = phi0, y = phi1),
    mapping = aes(x = x, y = y),
    color = "red3")
```
