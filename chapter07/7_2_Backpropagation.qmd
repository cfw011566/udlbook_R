---
title: "7.2 Backpropagation"
author: "CF Wang"
date: "2026-01-22"
format: html
---

```{r}
#| label: setup
#| include: false
#| fig-width: 6
#| fig-asp: 0.618
#| fig-align: center
#| out-width: 70%
```

This notebook runs the backpropagation algorithm on a deep neural network as described in section 7.4 of the book.

First let's define a neural network. We'll just choose the weights and biases randomly for now

```{r}
#| label: create-nerual-network

# Set seed so we always get the same random numbers
set.seed(0)

# Number of hidden layers
K <- 5
# Number of neurons per layer
D <- 6
# Input layer
D_i <- 1
# Output layer
D_o <- 1

# Make empty lists
all_weights <- vector(mode = "list", length = K + 1)
all_biases <- vector(mode = "list", length = K + 1)

# Create input and output layers
all_weights[[1]] <- matrix(rnorm(D*D_i), nrow = D, ncol = D_i)
all_weights[[K+1]] <- matrix(rnorm(D_o*D), nrow = D_o, ncol = D)
all_biases[[1]] <- matrix(rnorm(D*1), nrow = D, ncol = 1)
all_biases[[K+1]] <- matrix(rnorm(D_o*1), nrow = D_o, ncol = 1)

# Create intermediate layers
for (layer in 2:K) {
  all_weights[[layer]] <- matrix(rnorm(D*D), nrow = D, ncol = D)
  all_biases[[layer]] <- matrix(rnorm(D*1), nrow = D, ncol = 1)
}
```

```{r}
#| label: ReLU

# Define the Rectified Linear Unit (ReLU) function
ReLU <- function(preactivation) {
  ifelse(preactivation > 0, preactivation, 0)
}
```

Now let's run our random network. The weight matrices $\boldsymbol\Omega_{0...K}$ are the entries of the list "all_weights" and the biases $\boldsymbol\beta_{0...K}$ are the entries of the list "all_biases"

We know that we will need the preactivations $\text{f}_{0...K}$ and the activations $\text{h}_{1...K}$ for the forward pass of backpropagation, so we'll store and return these as well.
 
```{r}
#| label: compute-network-function

compute_network_output <- function(net_input, all_weights, all_biases) {
  # Retrieve number of layers
  K <- length(all_weights) - 1

  # We'll store the pre-activations at each layer in a list "all_f"
  # and the activations in a second list "all_h".
  all_f <- vector(mode = "list", length = K+1)
  all_h <- vector(mode = "list", length = K+1)

  #For convenience, we'll set
  # all_h[0] to be the input, and all_f[K] will be the output
  all_h[[1]] <- net_input

  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]
  for (layer in 1:K) {
      # Update preactivations and activations at this layer according to eqn 7.17
      # Remember to use np.matmul for matrix multiplications
      # TODO -- Replace the lines below
      all_f[[layer]] <- all_biases[[layer]] + all_weights[[layer]] %*% all_h[[layer]]
      all_h[[layer+1]] <- ReLU(all_f[[layer]])
  }
  # Compute the output from the last hidden layer
  # TODO -- Replace the line below
  all_f[K+1] <- all_biases[[K+1]] + all_weights[[K+1]] %*% all_h[[K+1]]

  # Retrieve the output
  net_output <- all_f[[K+1]]

  list(net_output = net_output, all_f = all_f, all_h = all_h)
}
```
 
```{r}
#| label: test-compute-network

# Define input
net_input <- matrix(1, nrow = D_i, ncol = 1) * 1.2
# Compute network output
network <- compute_network_output(net_input, all_weights, all_biases)
net_output <- network$net_output
all_f <- network$all_f
all_h <- network$all_h
#sprintf("True output = %3.3f, Your answer = %3.3f", 1.907, network$net_output)
sprintf("True output = %3.3f, Your answer = %3.3f", -0.577, network$net_output)
```

Now let's define a loss function. We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput

```{r}
#| label: loss-function

least_squares_loss <- function(net_output, y) {
  sum((net_output - y)^2)
}

d_loss_d_output <- function(net_output, y) {
  2 * (net_output - y)
}
```

```{r}
#| label: test-loss-function

y <- matrix(1, nrow = D_o, ncol = 1) * 20.0
loss <- least_squares_loss(network$net_output, y)
sprintf("y = %3.3f Loss = %3.3f", y, loss)
```
 
Now let's compute the derivatives of the network. We already computed the forward pass. Let's compute the backward pass. 

```{r}
#| label: indicator-and-backward-propagation

# We'll need the indicator function
indicator_function <- function(x) {
  x_in <- c(x)
  x_in[x_in>0] = 1
  x_in[x_in<=0] = 0
  x_in
}

# Main backward pass routine
backward_pass <- function(all_weights, all_biases, all_f, all_h, y) {
  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well
  all_dl_dweights <- vector(mode = "list", length = K+1)
  all_dl_dbiases <- vector(mode = "list", length = K+1)
  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists
  all_dl_df <- vector(mode = "list", length = K+1)
  all_dl_dh <- vector(mode = "list", length = K+1)
  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output

  # Compute derivatives of the loss with respect to the network output
  all_dl_df[[K+1]] <- c(d_loss_d_output(all_f[[K+1]], y))

  # Now work backwards through the network
  for (layer in rev(1:(K+1))) {
    # TODO Calculate the derivatives of the loss with respect to the biases at layer from all_dl_df[layer]. (eq 7.22)
    # NOTE!  To take a copy of matrix X, use Z=np.array(X)
    # REPLACE THIS LINE
    all_dl_dbiases[[layer]] <- all_dl_df[[layer]]

    # TODO Calculate the derivatives of the loss with respect to the weights at layer from all_dl_df[layer] and all_h[layer] (eq 7.23)
    # Don't forget to use np.matmul
    # REPLACE THIS LINE
    all_dl_dweights[[layer]] <- all_dl_df[[layer]] %*% t(all_h[[layer]])

    # TODO: calculate the derivatives of the loss with respect to the activations from weight and derivatives of next preactivations (second part of last line of eq 7.25)
    # REPLACE THIS LINE
    all_dl_dh[[layer]] <- t(all_weights[[layer]]) %*% all_dl_df[[layer]]


    if (layer > 1) {
      # TODO Calculate the derivatives of the loss with respect to the pre-activation f (use derivative of ReLu function, first part of last line of eq. 7.25)
      # REPLACE THIS LINE
      all_dl_df[[layer-1]] <- indicator_function(all_f[[layer-1]]) * all_dl_dh[[layer]]
    }
  }

  list(all_dl_dweights = all_dl_dweights, all_dl_dbiases = all_dl_dbiases)
}
```

```{r}
#| label: test-backward_pass

back_pass <- backward_pass(all_weights, all_biases, all_f, all_h, y)
all_dl_dweights <- back_pass$all_dl_dweights
all_dl_dbiases <- back_pass$all_dl_dbiases
```

```{r}
#| label: compare-with-finite-differences

# Make space for derivatives computed by finite differences
all_dl_dweights_fd <- vector(mode = "list", length = K+1)
all_dl_dbiases_fd <- vector(mode = "list", length = K+1)

# Let's test if we have the derivatives right using finite differences
delta_fd <- 0.000001

# Test the dervatives of the bias vectors
for (layer in 1:(K+1)) {
  dl_dbias <- all_dl_dbiases[[layer]]
  # For every element in the bias
  for (row in 1:nrow(all_biases[[layer]])) {
    # Take copy of biases  We'll change one element each time
    all_biases_copy <- all_biases
    all_biases_copy[[layer]][row] <- all_biases_copy[[layer]][row] + delta_fd
    network_output_1 <- compute_network_output(net_input, all_weights, all_biases_copy)
    network_output_2 <- compute_network_output(net_input, all_weights, all_biases)
    dl_dbias[row] <- (least_squares_loss(network_output_1$net_output, y) - least_squares_loss(network_output_2$net_output, y)) / delta_fd
  }
  all_dl_dbiases_fd[[layer]] <- dl_dbias
  print("-----------------------------------------------")
  print(sprintf("Bias %d, derivatives from backprop:", layer))
  print(all_dl_dbiases[[layer]])
  print(sprintf("Bias %d, derivatives from finite differences", layer))
  print(all_dl_dbiases_fd[[layer]])
  if (isTRUE(all.equal(all_dl_dbiases_fd[[layer]], all_dl_dbiases[[layer]], tolerance = 1e-07))) {
    print("Success!  Derivatives match.")
  } else {
    print("Failure!  Derivatives different.") 
  }
}

# Test the derivatives of the weights matrices
for (layer in 1:(K+1)) {
  dl_dweight <- all_dl_dweights[[layer]]
  # For every element in the bias
  for (row in nrow(all_weights[[layer]])) {
    for (col in ncol(all_weights[[layer]])) {
      # Take copy of biases  We'll change one element each time
      all_weights_copy <- all_weights
      all_weights_copy[[layer]][row][col] <- all_weights_copy[[layer]][row][col] + delta_fd
      network_output_1 <- compute_network_output(net_input, all_weights_copy, all_biases)
      network_output_2 <- compute_network_output(net_input, all_weights, all_biases)
      dl_dweight[row][col] <- (least_squares_loss(network_output_1$net_output, y) - least_squares_loss(network_output_2$net_output, y)) / delta_fd
    }
  }
  all_dl_dweights_fd[[layer]] <- dl_dweight
  print("-----------------------------------------------")
  print(sprintf("Weight %d, derivatives from backprop:", layer))
  print(all_dl_dweights[[layer]])
  print(sprintf("Weight %d, derivatives from finite differences", layer))
  print(all_dl_dweights_fd[[layer]])
  if (isTRUE(all.equal(all_dl_dweights_fd[[layer]], all_dl_dweights[[layer]], tolerance = 1e-07))) {
    print("Success!  Derivatives match.")
  } else {
    print("Failure!  Derivatives different.")
  }
}
```
