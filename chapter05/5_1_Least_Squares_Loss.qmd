---
title: "5.1 Least Squares Loss"
author: "CF Wang"
date: "2026-01-19"
format: html
---

```{r}
#| label: setup
#| include: false
#| fig-width: 6
#| fig-asp: 0.618
#| fig-align: center
#| out-width: 70%

library(ggplot2)
library(patchwork)
```

This notebook investigates the least squares loss and the equivalence of maximum likelihood and minimum negative log likelihood.

```{r}
#| label: ReLU-shallow-neural-network

# Define the Rectified Linear Unit (ReLU) function
ReLU <- function(preactivation) {
  ifelse(preactivation > 0, preactivation, 0)
}

# Define a shallow neural network
shallow_nn <- function(x, beta_0, omega_0, beta_1, omega_1) {
  # Make sure that input data is (1 x n_data) array
  n_data <- length(x)
  x <- matrix(x, nrow = 1, ncol = n_data)
  
  # This runs the network for ALL of the inputs, x at once so we can draw graph
  # h1 = ReLU(beta_0 %*% matrix(1, nrow = 1, ncol = n_data) + omega_0 %*% x)
  # y = beta_1 %*% matrix(1, nrow = 1, ncol = n_data) + omega_1 %*% h1
  # R is column oriental so c(beta_0) converts to column vector and add to matrix
  h1 <- ReLU(c(beta_0) + omega_0 %*% x)
  y <- c(beta_1) + omega_1 %*% h1
}
```

```{r}
#| label: default-parameters

# Get parameters for model -- we can call this function to easily reset them
get_parameters <- function() {
  # And we'll create a network that approximately fits it
  beta_0 <- matrix(0, nrow = 3, ncol = 1)  # formerly theta_x0
  omega_0 <- matrix(0, nrow = 3, ncol = 1) # formerly theta_x1
  beta_1 <- matrix(0, nrow = 1, ncol = 1)  # formerly phi_0
  omega_1 <- matrix(0, nrow = 1, ncol = 3) # formerly phi_x

  beta_0[1,1] <- 0.3; beta_0[2,1] <- -1.0; beta_0[3,1] <- -0.5
  omega_0[1,1] <- -1.0; omega_0[2,1] <- 1.8; omega_0[3,1] <- 0.65
  beta_1[1,1] <- 0.1;
  omega_1[1,1] <- -2.0; omega_1[1,2] <- -1.0; omega_1[1,3] <- 7.0

  list(beta_0 = beta_0, omega_0 = omega_0, beta_1 = beta_1, omega_1 = omega_1)
}
```

```{r}
#| label: plot-function

# Utility function for plotting data
plot_univariate_regression <- function(x_model, y_model, x_data = NULL, y_data = NULL, sigma_model = NULL, title = NULL) {
  y_model <- c(y_model)
  p0 <- ggplot() +
    xlab("Intput, x") + ylab("Output, y") +
    xlim(c(0, 1)) +
    scale_y_continuous(breaks = seq(-1, 1, by = 0.25), limits = c(-1, 1)) +
    ggtitle(title) +
    theme(plot.title = element_text(hjust = 0.5)) 
  
  if (!is.null(sigma_model)) {
    y_points <- c(y_model+2*sigma_model, rev(y_model)-2*sigma_model)
    y_points <- ifelse(y_points > 1, 1, y_points)
    y_points <- ifelse(y_points < -1, -1, y_points)
    p0 <- p0 + geom_polygon(
      mapping = aes(x = c(x_model, rev(x_model)), y = y_points),
      fill = "lightgray"
    )
  }
  
  if (!is.null(x_data)) {
    p0 <- p0 +
      geom_point(mapping = aes(x = x_data, y = y_data), color = "black", size = 2)
  }
  
  p0 + geom_line(mapping = aes(x = x_model, y = y_model), color = "skyblue3", linewidth = 0.75)
}
```

# Univariate regression

We'll investigate a simple univariate regression situation with a single input $x$ and a single output $y$ as pictured in figures 5.4 and 5.5b.
 
```{r}
#| label: univariate-regression
#| fig-asp: 1

# Let's create some 1D training data
x_train <- c(0.09291784, 0.46809093, 0.93089486, 0.67612654, 0.73441752, 0.86847339, 0.49873225, 0.51083168, 0.18343972, 0.99380898, 0.27840809, 0.38028817, 0.12055708, 0.56715537, 0.92005746, 0.77072270, 0.85278176, 0.05315950, 0.87168699, 0.58858043)
y_train <- c(-0.25934537, 0.18195445, 0.651270150, 0.13921448, 0.09366691, 0.30567674, 0.372291170, 0.20716968, -0.08131792, 0.51187806, 0.16943738, 0.3994327, 0.019062570, 0.55820410, 0.452564960, -0.1183121, 0.02957665, -1.24354444, 0.248038840, 0.26824970)

# Get parameters for the model
# beta_0, omega_0, beta_1, omega_1 = get_parameters()
params <- get_parameters()
sigma <- 0.2

# Define a range of input values
x_model <- seq(0, 1, 0.01)
# Run the model to get values to plot and plot it.
# y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)
y_model <- shallow_nn(x_model, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma)
```

The blue line is the mean prediction of the model and the gray area represents plus/minus two standard deviations. This model fits okay, but could be improved. Let's compute the loss. We'll compute the the least squares error, the likelihood, the negative log likelihood.

```{r}
#| label: normal-distribution-function

# Return probability under normal distribution
normal_distribution <- function(y, mu, sigma) {
  # TODO-- write in the equation for the normal distribution
  # Equation 5.7 from the notes (you will need np.sqrt() and np.exp(), and math.pi)
  # Don't use the numpy version -- that's cheating!
  # dnorm(y, mu, sigma)
  # Replace the line below
  exp(-(y-mu)^2 / 2.0 / sigma^2) / sqrt(2*pi*sigma^2)
}

# Let's double check we get the right answer before proceeding
sprintf("Correct answer = %3.3f, Your answer = %3.3f", 0.119, normal_distribution(1, -1, 2.3))
```

```{r}
#| label: plot-normal-distribution

# Let's plot the Gaussian distribution.
y_gauss <- seq(-5, 5, 0.1)
mu <- 0; sigma <- 1.0
gauss_prob <- normal_distribution(y_gauss, mu, sigma)

ggplot() +
  xlim(c(-5, 5)) + ylim(c(0, 1)) +
  xlab(expression("Input, "*y)) +
  ylab(expression("Probability "*Pr(y))) +
  geom_line(mapping = aes(x = y_gauss, y = gauss_prob), color = "skyblue3", linewidth = 0.75)
  
# TODO
# 1. Predict what will happen if we change to mu=1 and leave sigma=1
# Now change the code above and see if you were correct.

# 2. Predict what will happen if we leave mu = 0 and change sigma to 2.0

# 3. Predict what will happen if we leave mu = 0 and change sigma to 0.5
```

Now let's compute the likelihood using this function

```{r}
#| label: likelihood

# Return the likelihood of all of the data under the model
compute_likelihood <- function(y_train, mu, sigma) {
  # TODO -- compute the likelihood of the data -- the product of the normal probabilities for each data point
  # Top line of equation 5.3 in the notes
  # You will need np.prod() and the normal_distribution function you used above
  # Replace the line below
  prod(normal_distribution(y_train, mu, sigma))
}

# Let's test this for a homoscedastic (constant sigma) model
params <- get_parameters()
# Use our neural network to predict the mean of the Gaussian
mu_pred <- shallow_nn(x_train, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
# Set the standard deviation to something reasonable
sigma <- 0.2
# Compute the likelihood
likelihood <- compute_likelihood(y_train, mu_pred, sigma)
# Let's double check we get the right answer before proceeding
sprintf("Correct answer = %9.9f, Your answer = %9.9f", 0.000010624, likelihood)
```

You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well. This is because it is the product of several probabilities, which are all quite small themselves. This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math

This is why we use negative log likelihood

```{r}
#| label: negative-log-likelihood

# Return the negative log likelihood of the data under the model
compute_negative_log_likelihood <- function(y_train, mu, sigma) {
  # TODO -- compute the negative log likelihood of the data without using a product
  # In other words, compute minus one times the sum of the log probabilities
  # Equation 5.4 in the notes
  # You will need np.sum(), np.log()
  # Replace the line below
  sum(-log(normal_distribution(y_train, mu, sigma)))
}
     
# Let's test this for a homoscedastic (constant sigma) model
params <- get_parameters()
# Use our neural network to predict the mean of the Gaussian
mu_pred <- shallow_nn(x_train, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
# Set the standard deviation to something reasonable
sigma <- 0.2
# Compute the negative log likelihood
nll <- compute_negative_log_likelihood(y_train, mu_pred, sigma)
# Let's double check we get the right answer before proceeding
sprintf("Correct answer = %9.9f, Your answer = %9.9f", 11.452419564, nll)
```

For good measure, let's compute the sum of squares as well

```{r}
#| label: squared-distance

# Return the squared distance between the observed data (y_train) and the prediction of the model (y_pred)
compute_sum_of_squares <- function(y_train, y_pred) {
  # TODO -- compute the sum of squared distances between the training data and the model prediction
  # Eqn 5.10 in the notes.  Make sure that you understand this, and ask questions if you don't
  # Replace the line below
  sum((y_train - y_pred)^2)
}
     
# Let's test this again
params <- get_parameters()
# Use our neural network to predict the mean of the Gaussian, which is out best prediction of y
mu_pred <- shallow_nn(x_train, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
y_pred <- mu_pred
# Compute the sum of squares
sum_of_squares <- compute_sum_of_squares(y_train, y_pred)
# Let's double check we get the right answer before proceeding
sprintf("Correct answer = %9.9f, Your answer = %9.9f", 2.020992572, sum_of_squares)
```

Now let's investigate finding the maximum likelihood / minimum negative log likelihood / least squares solution. For simplicity, we'll assume that all the parameters are correct except one and look at how the likelihood, negative log likelihood, and sum of squares change as we manipulate the last parameter. We'll start with overall y offset, beta_1 (formerly phi_0)

```{r}
#| label: comparison-for-different-beta_1
#| fig-asp: 4

# Define a range of values for the parameter
beta_1_vals <- seq(0, 1.0, 0.01)
# Create some arrays to store the likelihoods, negative log likelihoods and sum of squares
likelihoods <- rep(0, times = length(beta_1_vals))
nlls <- rep(0, times = length(beta_1_vals))
sum_squares <- rep(0, times = length(beta_1_vals))

# Initialise the parameters
params <- get_parameters()
sigma <- 0.2
p <- NULL
for (count in 1:length(beta_1_vals)) {
  # Set the value for the parameter
  params$beta_1[1,1] = beta_1_vals[count]
  # Run the network with new parameters
  y_pred <- shallow_nn(x_train, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
  mu_pred <- y_pred
  # Compute and store the three values
  likelihoods[count] <- compute_likelihood(y_train, mu_pred, sigma)
  nlls[count] <- compute_negative_log_likelihood(y_train, mu_pred, sigma)
  sum_squares[count] <- compute_sum_of_squares(y_train, y_pred)
  # Draw the model for every 20th parameter setting
  if ((count-1) %% 20 == 0) {
    # Run the model to get values to plot and plot it.
    y_model <- shallow_nn(x_model, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
    p <- p / plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title = sprintf("beta1=%.3f", params$beta_1[1,1]))
  }
}
p
```

```{r}
#| fig-width: 8
#| fig-asp: 0.5

# Now let's plot the likelihood, negative log likelihood, and least squares as a function of the value of the offset beta1
likelihood_color <- "red3"
nll_color <- "skyblue3"
scale <- max(nlls) / max(likelihoods)

p1 <- ggplot() +
  xlab("beta_1") +
  scale_y_continuous(sec.axis = sec_axis(~.*scale, name = "negative log likelihood", breaks = seq(0, 140, 20)))+
  geom_line(aes(x = beta_1_vals, y = likelihoods), color = likelihood_color) +
  geom_line(aes(x = beta_1_vals, y = nlls/scale), color = nll_color) +
  geom_vline(xintercept = beta_1_vals[which.max(likelihoods)], color = nll_color, linetype = 3) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.y.right = element_text(color = nll_color),
        axis.ticks.y.right = element_line(color = nll_color),
        axis.title.y.right = element_text(color = nll_color),
        axis.text.y = element_text(color = likelihood_color),
        axis.ticks.y = element_line(color = likelihood_color),
        axis.title.y = element_text(color = likelihood_color))

p2 <- ggplot() +
  ylim(c(1, 12)) +
  xlab("beta_1") + ylab("sum of squares") +
  geom_line(aes(x = beta_1_vals, y = sum_squares), color = "skyblue3")

p1 + p2
```

```{r}
#| label: best_beta_1

# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood
# and the least squares solutions
# Let's check that:
sprintf("Maximum likelihood = %3.3f, at beta_1 = %3.3f", likelihoods[which.max(likelihoods)], beta_1_vals[which.max(likelihoods)])
sprintf("Minimum negative log likelihood = %3.3f, at beta_1 = %3.3f", nlls[which.min(nlls)], beta_1_vals[which.min(nlls)])
sprintf("Least squares = %3.3f, at beta_1 = %3.3f", sum_squares[which.min(sum_squares)], beta_1_vals[which.min(sum_squares)])

# Plot the best model
params$beta_1[1,1] = beta_1_vals[which.min(sum_squares)]
y_model <- shallow_nn(x_model, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title = sprintf("beta1=%3.3f", params$beta_1[1,1]))
```

They all give the same answer. But you can see from the three plots above that the likelihood is very small unless the parameters are almost correct. So in practice, we would work with the negative log likelihood or the least squares.

Let's do the same thing with the standard deviation parameter of our network. This is not an output of the network (unless we choose to make that the case), but it still affects the likelihood.

```{r}
#| label: comparison with different sigma
#| fig-asp: 4

# Define a range of values for the parameter
sigma_vals <- seq(0.1, 0.5, 0.005)
# Create some arrays to store the likelihoods, negative log likelihoods and sum of squares
likelihoods <- rep(0, times = length(sigma_vals))
nlls <- rep(0, times = length(sigma_vals))
sum_squares <- rep(0, times = length(sigma_vals))

# Initialise the parameters
params <- get_parameters()
# Might as well set to the best offset
params$beta_1[1,1] = 0.27
p <- NULL
for (count in 1:length(sigma_vals)) {
  # Set the value for the parameter
  sigma <- sigma_vals[count]
  # Run the network with new parameters
  y_pred <- shallow_nn(x_train, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
  mu_pred <- y_pred
  # Compute and store the three values
  likelihoods[count] <- compute_likelihood(y_train, mu_pred, sigma)
  nlls[count] <- compute_negative_log_likelihood(y_train, mu_pred, sigma)
  sum_squares[count] <- compute_sum_of_squares(y_train, y_pred)
  # Draw the model for every 20th parameter setting
  if ((count-1) %% 20 == 0) {
    # Run the model to get values to plot and plot it.
    y_model <- shallow_nn(x_model, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
    p <- p / plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title = sprintf("sigma=%.3f", sigma))
  }
}
p
```

```{r}
#| fig-width: 8
#| fig-asp: 0.5

# Now let's plot the likelihood, negative log likelihood, and least squares as a function of the value of the standard deviation sigma
likelihood_color <- "red3"
nll_color <- "skyblue3"
scale <- max(nlls) / max(likelihoods)

p1 <- ggplot() +
  xlab("sigma") +
  scale_y_continuous(sec.axis = sec_axis(~.*scale, name = "negative log likelihood", breaks = seq(0, 50, 10)))+
  geom_line(aes(x = sigma_vals, y = likelihoods), color = likelihood_color) +
  geom_line(aes(x = sigma_vals, y = nlls/scale), color = nll_color) +
  geom_vline(xintercept = sigma_vals[which.max(likelihoods)], color = nll_color, linetype = 3) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.y.right = element_text(color = nll_color),
        axis.ticks.y.right = element_line(color = nll_color),
        axis.title.y.right = element_text(color = nll_color),
        axis.text.y = element_text(color = likelihood_color),
        axis.ticks.y = element_line(color = likelihood_color),
        axis.title.y = element_text(color = likelihood_color))

p2 <- ggplot() +
  ylim(range(sum_squares)) +
  xlab("sigma") + ylab("sum of squares") +
  geom_line(aes(x = sigma_vals, y = sum_squares), color = "skyblue3")

p1 + p2
```

```{r}
#| label: best-sigma

# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood
# The least squares solution does not depend on sigma, so it's just flat -- no use here.
# Let's check that:
sprintf("Maximum likelihood = %3.3f, at sigma = %3.3f", likelihoods[which.max(likelihoods)], sigma_vals[which.max(likelihoods)])
sprintf("Minimum negative log likelihood = %3.3f, at sigma = %3.3f", nlls[which.min(nlls)],sigma_vals[which.min(nlls)])
# Plot the best model
sigma <- sigma_vals[which.min(nlls)]
y_model <- shallow_nn(x_model, params$beta_0, params$omega_0, params$beta_1, params$omega_1)
plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title = sprintf("beta_1=%.3f, sigma=%.3f", params$beta_1[1,1], sigma))
```
