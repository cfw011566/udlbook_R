---
title: "6.3 Stochastic Gradient Descent"
author: "CF Wang"
date: "2026-01-20"
format: html
---

```{r}
#| label: setup
#| include: false
#| fig-width: 6
#| fig-asp: 0.618
#| fig-align: center
#| out-width: 70%

library(ggplot2)
```

This notebook investigates gradient descent and stochastic gradient descent and recreates figure 6.5 from the book

```{r}
#| label: dataset

# Let's create our training data of 30 pairs {x_i, y_i}
# We'll try to fit the Gabor model to these data
data <- matrix(c(-1.920e+00, -1.422e+01, 1.490e+00, -1.940e+00, -2.389e+00, -5.090e+00,
                 -8.861e+00, 3.578e+00, -6.010e+00, -6.995e+00, 3.634e+00, 8.743e-01,
                 -1.096e+01, 4.073e-01, -9.467e+00, 8.560e+00, 1.062e+01, -1.729e-01,
                  1.040e+01, -1.261e+01, 1.574e-01, -1.304e+01, -2.156e+00, -1.210e+01,
                 -1.119e+01, 2.902e+00, -8.220e+00, -1.179e+01, -8.391e+00, -4.505e+00,
                 -1.051e+00, -2.482e-02, 8.896e-01, -4.943e-01, -9.371e-01, 4.306e-01,
                  9.577e-03, -7.944e-02, 1.624e-01, -2.682e-01, -3.129e-01, 8.303e-01,
                 -2.365e-02, 5.098e-01, -2.777e-01, 3.367e-01, 1.927e-01, -2.222e-01,
                  6.352e-02, 6.888e-03, 3.224e-02, 1.091e-02, -5.706e-01, -5.258e-02,
                 -3.666e-02, 1.709e-01, -4.805e-02, 2.008e-01, -1.904e-01, 5.952e-01),
               nrow = 2, byrow = TRUE)
```

```{r}
#| label: Gabor-model

# Let's define our model
model <- function(phi, x) {
  sin_component <- sin(phi[1] + 0.06 * phi[2] * x)
  gauss_component <- exp(-(phi[1] + 0.06 * phi[2] * x)^2 / 32)
  y_pred <- sin_component * gauss_component
}
```

```{r}
#| label: draw-model

# Draw model
draw_model <- function(data, model, phi, title = NULL) {
  x_model <- seq(-15, 15, 0.1)
  y_model <- model(phi, x_model)

  ggplot() +
    xlab("x") + ylab("y") +
    ylim(c(-1, 1)) +
    scale_x_continuous(breaks = seq(-15, 15, by = 5)) +
    geom_line(aes(x = x_model, y = y_model), color = "magenta3", linewidth = 0.75) +
    geom_point(aes(x = data[1,], y = data[2,]), color = "blue3", size = 2) +
    ggtitle(title) +
    theme(plot.title = element_text(hjust = 0.5)) 
}
```

```{r}
#| fig-asp: 1

# Initialize the parameters and draw the model
#phi = np.zeros((2,1))
#phi[0] =  -5     # Horizontal offset
#phi[1] =  25     # Frequency
phi <- c(-5, 25)
draw_model(data, model, phi, "Initial parameters")
```

Now let's compute the sum of squares loss for the training data

```{r}
#| label: compute-loss

compute_loss <- function(data_x, data_y, model, phi) {
  # TODO -- Write this function -- replace the line below
  # TODO -- First make model predictions from data x
  # TODO -- Then compute the squared difference between the predictions and true y values
  # TODO -- Then sum them all and return
  pred_y <- model(phi, data_x)
  loss <- sum((pred_y - data_y)^2)
}
```

Let's just test that we got that right

```{r}
#| label: test-compute-loss

loss <- compute_loss(data[1,], data[2,], model, c(0.6,-0.2))
sprintf("Your loss = %3.3f, Correct loss = %3.3f", loss, 16.419)
```

Now let's plot the whole loss function

```{r}
#| label: draw-loss
#| fig-asp: 1

draw_loss_function <- function(compute_loss, data,  model, phi_iters = NULL) {
  phi0_range <- seq(-10, 10, 0.1)
  phi1_range <- seq(2.5, 22.5, 0.1)
  all_losses <- outer(phi0_range, phi1_range)
  for (i in 1:length(phi0_range)) {
    for (j in 1:length(phi1_range)) {
      all_losses[i, j] = compute_loss(data[1,], data[2,], model, c(phi0_range[i], phi1_range[j]))
    }
  }
# all_losses <- outer(phi0_range, phi1_range, function(p0, p1) { compute_loss(x, y, p0, p1)})

  df <- expand.grid(phi0 = phi0_range, phi1 = phi1_range)
  df$loss <- c(all_losses)
  p <- ggplot() +
    xlim(-10, 10) +
    ylim(2.5, 22.5) +
    xlab(expression("Intercep, "*phi[0])) +
    ylab(expression("Slope, "*phi[1])) +
    geom_contour_filled(
      data = df,
      mapping = aes(x = phi0, y = phi1, z = loss),
      show.legend = FALSE,
      bins = 128) +
    geom_contour(
      data = df,
      mapping = aes(x = phi0, y = phi1, z = loss),
      show.legend = FALSE,
      bins = 20,
      color = "grey",
      linewidth = 0.25)
  if (!is.null(phi_iters)) {
    color_rgb <- col2rgb("red") / 255
    points_color <- rgb(red = color_rgb[1], green = color_rgb[2], blue = color_rgb[3], alpha = seq(0.2, 1, length.out = ncol(phi_iters)))
  
    p <- p + geom_point(
      mapping = aes(x = phi_iters[1,], y = phi_iters[2,]),
      color = points_color
    )
    p <- p + geom_path(aes(x = phi_iters[1,], y = phi_iters[2,]), color = "red3")
  }
  p
}

draw_loss_function(compute_loss, data, model)
```

Now let's compute the gradient vector for a given set of parameters:
$$
\frac{\partial L}{\partial\phi} =
\begin{bmatrix}
\frac{\partial L}{\partial\phi_0} \\
\frac{\partial L}{\partial \phi_1}
\end{bmatrix}
\tag{1}
$$

```{r}
#| label: compute-gradient

# These came from writing out the expression for the sum of squares loss and taking the
# derivative with respect to phi0 and phi1. It was a lot of hassle to get it right!
gabor_deriv_phi0 <- function(data_x, data_y, phi0, phi1) {
  x <- 0.06 * phi1 * data_x + phi0
  y <- data_y
  cos_component <- cos(x)
  sin_component <- sin(x)
  gauss_component <- exp(-0.5 * x * x / 16)
  deriv <- cos_component * gauss_component - sin_component * gauss_component * x / 16
  deriv <- 2 * deriv * (sin_component * gauss_component - y)
  sum(deriv)
}

gabor_deriv_phi1 <- function(data_x, data_y, phi0, phi1) {
  x <- 0.06 * phi1 * data_x + phi0
  y <- data_y
  cos_component <- cos(x)
  sin_component <- sin(x)
  gauss_component <- exp(-0.5 * x * x / 16)
  deriv <- 0.06 * data_x * cos_component * gauss_component - 0.06 * data_x * sin_component * gauss_component * x / 16
  deriv <- 2 * deriv * (sin_component * gauss_component - y)
  sum(deriv)
}

compute_gradient <- function(data_x, data_y, phi) {
  dl_dphi0 <- gabor_deriv_phi0(data_x, data_y, phi[1], phi[2])
  dl_dphi1 <- gabor_deriv_phi1(data_x, data_y, phi[1], phi[2])
  # Return the gradient
  c(dl_dphi0, dl_dphi1)
}
```

We can check we got this right using a trick known as **finite differences**. If we evaluate the function and then change one of the parameters by a very small amount and normalize by that amount, we get an approximation to the gradient, so:
$$
\begin{aligned}
\frac{\partial L}{\partial\phi_0} &\approx \frac{L[\phi_0+\delta,\phi_1] - L[\phi_0,\phi_1]}{\delta} \\
\frac{\partial L}{\partial\phi_1} &\approx \frac{L[\phi_0,\phi_1+\delta] - L[\phi_0,\phi_1]}{\delta}
\end{aligned}
$$
We can't do this when there are many parameters; for a million parameters, we would have to evaluate the loss function two million times, and usually computing the gradients directly is much more efficient.

```{r}
#| label: test-compute-gradient

# Compute the gradient using your function
gradient <- compute_gradient(data[1,], data[2,], phi)
sprintf("Your gradients: (%3.3f,%3.3f)", gradient[1], gradient[2])
# Approximate the gradients with finite differences
delta <- 0.0001
dl_dphi0_est <- (compute_loss(data[1,], data[2,], model, phi+c(delta, 0)) - compute_loss(data[1,], data[2,], model, phi)) / delta
dl_dphi1_est <- (compute_loss(data[1,], data[2,], model, phi+c(0, delta)) - compute_loss(data[1,], data[2,], model, phi)) / delta
sprintf("Approx gradients: (%3.3f,%3.3f)", dl_dphi0_est, dl_dphi1_est)
```

Now we are ready to perform gradient descent. We'll need to use our line search routine from Notebook 6.1, which I've reproduced here plus the helper function loss_function_1D that converts from a 2D problem to a 1D problem

```{r}
#| label: line-search

loss_function_1D <- function(dist_prop, data, model, phi_start, search_direction) {
  # Return the loss after moving this far
  compute_loss(data[1,], data[2,], model, phi_start - search_direction * dist_prop)
}

line_search <- function(data, model, phi, gradient, thresh=.00001, max_dist = 0.1, max_iter = 15, verbose = FALSE) {
  # Initialize four points along the range we are going to search
  a <- 0
  b <- 0.33 * max_dist
  c <- 0.66 * max_dist
  d <- 1.0 * max_dist
  n_iter <- 0

  # While we haven't found the minimum closely enough
  while (abs(b-c) > thresh && n_iter < max_iter) {
    # Increment iteration counter (just to prevent an infinite loop)
    n_iter <- n_iter+1
    # Calculate all four points
    lossa <- loss_function_1D(a, data, model, phi, gradient)
    lossb <- loss_function_1D(b, data, model, phi, gradient)
    lossc <- loss_function_1D(c, data, model, phi, gradient)
    lossd <- loss_function_1D(d, data, model, phi, gradient)

    if (verbose) {
      print(sprintf("Iter %d, a=%3.3f, b=%3.3f, c=%3.3f, d=%3.3f", n_iter, a, b, c, d))
      print(sprintf("a%f, b%f, c%f, d%f", lossa, lossb, lossc, lossd))
    }

    # Rule #1 If point A is less than points B, C, and D then halve distance from A to points B,C, and D
    if (min(lossa,lossb,lossc,lossd) == 0) {
      b <- a + (b-a) / 2
      c <- a + (c-a) / 2
      d <- a + (d-a) / 2
      next
    }

    # Rule #2 If point b is less than point c then
    #                     point d becomes point c, and
    #                     point b becomes 1/3 between a and new d
    #                     point c becomes 2/3 between a and new d
    if (lossb < lossc) {
      d <- c
      b <- a + (d-a) / 3
      c <- a + 2 * (d-a) / 3
      next
    }

    # Rule #2 If point c is less than point b then
    #                     point a becomes point b, and
    #                     point b becomes 1/3 between new a and d
    #                     point c becomes 2/3 between new a and d
    a <- b
    b <- a + (d-a) / 3
    c <- a + 2 * (d-a) / 3
  }

  # Return average of two middle points
  (b + c) / 2.0
}
```

```{r}
#| label: gradient-descent-step

gradient_descent_step <- function(phi, data, model) {
  # Step 1:  Compute the gradient
  gradient <- compute_gradient(data[1,], data[2,], phi)
  # Step 2:  Update the parameters -- note we want to search in the negative (downhill direction)
  alpha <- line_search(data, model, phi, gradient*-1, max_dist = 2.0)
  phi <- phi - alpha * gradient
}
```

```{r}
#| fig-asp: 1

# Initialize the parameters
n_steps <- 21
phi_all <- matrix(0, nrow = 2, ncol = n_steps+1)
phi_all[1,1] <- -1.5
phi_all[2,1] <- 8.5

# Measure loss and draw initial model
loss <- compute_loss(data[1,], data[2,], model, phi_all[,1])
draw_model(data, model, phi_all[,1], sprintf("Initial parameters, Loss = %f", loss))

for (c_step in 1:n_steps) {
  # Do gradient descent step
  phi_all[,c_step+1] <- gradient_descent_step(phi_all[,c_step], data, model)
  # Measure loss and draw model
  if (c_step %% 4 == 0) {
    loss <- compute_loss(data[1,], data[2,], model, phi_all[,c_step+1])
    p <- draw_model(data, model, phi_all[,c_step+1], sprintf("Iteration %d, loss = %f", c_step, loss))
    print(p)
  }
}

draw_loss_function(compute_loss, data, model, phi_all)
```

```{r}
# TODO Experiment with starting the optimization in the previous cell in different places
# and show that it heads to a local minimum if we don't start it in the right valley
```

```{r}
#| label: gradient-descent-step-fixed-learning-rate

gradient_descent_step_fixed_learning_rate <- function(phi, data, alpha) {
  # TODO -- fill in this routine so that we take a fixed size step of size alpha without using line search
  gradient <- compute_gradient(data[1,], data[2,], phi)
  phi <- phi - alpha * gradient
}
```

```{r}
#| fig-asp: 1

# Initialize the parameters
n_steps <- 21
phi_all <- matrix(0, nrow = 2, ncol = n_steps+1)
phi_all[1,1] <- -1.5
phi_all[2,1] <- 8.5

# Measure loss and draw initial model
loss <- compute_loss(data[1,], data[2,], model, phi_all[,1])
draw_model(data, model, phi_all[,1], sprintf("Initial parameters, Loss = %f", loss))

for (c_step in 1:n_steps) {
  # Do gradient descent step
  phi_all[,c_step+1] <- gradient_descent_step_fixed_learning_rate(phi_all[,c_step], data, alpha = 0.2)
  # Measure loss and draw model
  if (c_step %% 4 == 0) {
    loss <- compute_loss(data[1,], data[2,], model, phi_all[,c_step+1])
    p <- draw_model(data, model, phi_all[,c_step+1], sprintf("Iteration %d, loss = %f", c_step, loss))
    print(p)
  }
}

draw_loss_function(compute_loss, data, model, phi_all)
```

```{r}
#| label: stochastic-gradient_descent-step

# TODO Experiment with the learning rate, alpha.
# What happens if you set it too large?
# What happens if you set it too small?
```

```{r}
stochastic_gradient_descent_step <- function(phi, data, alpha, batch_size) {
  # TODO -- fill in this routine so that we take a fixed size step of size alpha but only using a subset (batch) of the data
  # at each step
  # You can use the function np.random.permutation to generate a random permutation of the n_data = data.shape[1] indices
  # and then just choose the first n=batch_size of these indices.  Then compute the gradient update
  # from just the data with these indices.   More properly, you should sample without replacement, but this will do for now.
  batch_index <- sample(ncol(data), batch_size)
  batch_data <- data[,batch_index]
  gradient <- compute_gradient(batch_data[1,], batch_data[2,], phi)
  phi <- phi - alpha * gradient
}
```

```{r}
#| fig-asp: 1

# Set the random number generator so you always get same numbers (disable if you don't want this)
set.seed(1)
# Initialize the parameters
n_steps <- 41
phi_all <- matrix(0, nrow = 2, ncol = n_steps+1)
phi_all[1,1] <- 3.5
phi_all[2,1] <- 6.5

# Measure loss and draw initial model
loss <- compute_loss(data[1,], data[2,], model, phi_all[,1])
draw_model(data, model, phi_all[,1], sprintf("Initial parameters, Loss = %f", loss))

for (c_step in 1:n_steps) {
  # Do gradient descent step
  phi_all[,c_step+1] <- stochastic_gradient_descent_step(phi_all[,c_step], data, alpha = 0.8, batch_size = 5)
  # Measure loss and draw model every 8th step
  if (c_step %% 8 == 0) {
    loss <- compute_loss(data[1,], data[2,], model, phi_all[,c_step+1])
    p <- draw_model(data, model, phi_all[,c_step+1], sprintf("Iteration %d, loss = %f", c_step, loss))
    print(p)
  }
}

draw_loss_function(compute_loss, data, model, phi_all)
```

```{r}
#| fig-asp: 1

# TODO -- Experiment with different learning rates, starting points, batch sizes, number of steps.  Get a feel for this.
set.seed(1)
n_steps <- 61
phi_all <- matrix(0, nrow = 2, ncol = n_steps+1)
phi_all[1,1] <- 2
phi_all[2,1] <- 5 

for (c_step in 1:n_steps) {
  # Do gradient descent step
  phi_all[,c_step+1] <- stochastic_gradient_descent_step(phi_all[,c_step], data, alpha = 0.5, batch_size = 10)
  # Measure loss and draw model every 8th step
  if (c_step %% 8 == 0) {
    loss <- compute_loss(data[1,], data[2,], model, phi_all[,c_step+1])
    print(sprintf("Iteration %d, loss = %f", c_step, loss))
  }
}
draw_model(data, model, phi_all[,c_step+1])

draw_loss_function(compute_loss, data, model, phi_all)
```
     
```{r}
#| fig-asp: 1

# TODO -- Add a learning rate schedule.  Reduce the learning rate by a factor of beta every M iterations
set.seed(1)
n_steps <- 61
phi_all <- matrix(0, nrow = 2, ncol = n_steps+1)
phi_all[1,1] <- 2 
phi_all[2,1] <- 5 

M <- ncol(data) / 10
alpha <- 0.9
for (c_step in 1:n_steps) {
  # Do gradient descent step
  phi_all[,c_step+1] <- stochastic_gradient_descent_step(phi_all[,c_step], data, alpha = alpha, batch_size = 10)
  # Measure loss and draw model every 8th step
  if (c_step %% M == 0) {
    alpha <- alpha * 0.9
  }
  if (c_step %% 8 == 0) {
    loss <- compute_loss(data[1,], data[2,], model, phi_all[,c_step+1])
    print(sprintf("Iteration %d, loss = %f", c_step, loss))
  }
}
draw_model(data, model, phi_all[,c_step+1])

draw_loss_function(compute_loss, data, model, phi_all)
```
